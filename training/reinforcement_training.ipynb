{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "764d78c7",
   "metadata": {},
   "source": [
    "# Reinforce Algorthim Training\n",
    "\n",
    "This notebook implements the REINFORCE (Monte Carlo Policy Gradient) algorithm to train an agent for the fire-rescue mission environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09820843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Imports and Setup\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from environment.custom_env import FireRescueEnv\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"../models/reinforce\", exist_ok=True)\n",
    "os.makedirs(\"../models/reinforce/best_model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49430ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Policy Network Architecture\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network for policy approximation in REINFORCE.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[128, 64]):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9407e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: REINFORCE Agent Implementation\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE algorithm implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, \n",
    "                 gamma=0.99, hidden_dims=[128, 64], entropy_coef=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        if training:\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            self.saved_log_probs.append(m.log_prob(action))\n",
    "            self.entropies.append(m.entropy())\n",
    "            return action.item()\n",
    "        else:\n",
    "            return torch.argmax(probs).item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using REINFORCE algorithm.\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G, entropy in zip(self.saved_log_probs, returns, self.entropies):\n",
    "            policy_loss.append(-log_prob * G - self.entropy_coef * entropy)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        loss_value = loss.item()\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model.\"\"\"\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Training Function\n",
    "def train_reinforce(env, agent, num_episodes=2000, save_path=None, \n",
    "                   config_name=\"default\", verbose_freq=100):\n",
    "    \"\"\"Train REINFORCE agent.\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    wall_collision_rates = []\n",
    "    scan_efficiencies = []\n",
    "    pickup_attempts_list = []\n",
    "    time_to_find_survivor_list = []\n",
    "    loss_history = []\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    \n",
    "    best_avg_reward = -np.inf\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        # Collect episode\n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            state = next_state\n",
    "        \n",
    "        # Update policy\n",
    "        loss = agent.update()\n",
    "        \n",
    "        # Track metrics from environment info\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(info.get('episode_length', episode_length))\n",
    "        wall_collision_rates.append(info.get('wall_collision_rate', 0))\n",
    "        scan_efficiencies.append(info.get('scan_efficiency', 0))\n",
    "        pickup_attempts_list.append(info.get('pickup_attempts', 0))\n",
    "        \n",
    "        time_found = info.get('time_to_find_survivor', None)\n",
    "        if time_found is not None:\n",
    "            time_to_find_survivor_list.append(time_found)\n",
    "        \n",
    "        recent_rewards.append(episode_reward)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        avg_reward = np.mean(recent_rewards)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_reward > best_avg_reward and episode > 100:\n",
    "            best_avg_reward = avg_reward\n",
    "            if save_path:\n",
    "                agent.save(save_path)\n",
    "        \n",
    "        # Logging\n",
    "        if episode % verbose_freq == 0:\n",
    "            recent_wall_collisions = np.mean(wall_collision_rates[-100:])\n",
    "            recent_scan_eff = np.mean(scan_efficiencies[-100:])\n",
    "            print(f\"Episode {episode}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Wall Collisions: {recent_wall_collisions:.2%} | \"\n",
    "                  f\"Scan Eff: {recent_scan_eff:.2%} | \"\n",
    "                  f\"Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Save final metrics\n",
    "    metrics = {\n",
    "        'config_name': config_name,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'wall_collision_rates': wall_collision_rates,\n",
    "        'scan_efficiencies': scan_efficiencies,\n",
    "        'pickup_attempts': pickup_attempts_list,\n",
    "        'time_to_find_survivor': time_to_find_survivor_list,\n",
    "        'loss_history': loss_history,\n",
    "        'best_avg_reward': best_avg_reward,\n",
    "        'avg_wall_collision_rate': float(np.mean(wall_collision_rates)),\n",
    "        'avg_scan_efficiency': float(np.mean(scan_efficiencies)),\n",
    "        'avg_pickup_attempts': float(np.mean(pickup_attempts_list)),\n",
    "        'avg_time_to_find_survivor': float(np.mean(time_to_find_survivor_list)) if time_to_find_survivor_list else None\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Hyperparameter Configurations\n",
    "HYPERPARAMETER_CONFIGS = [\n",
    "    {\n",
    "        'name': 'config_1_baseline',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_2_high_lr',\n",
    "        'learning_rate': 0.003,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_3_low_lr',\n",
    "        'learning_rate': 0.0005,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_4_high_gamma',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.995,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_5_low_gamma',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.95,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_6_deep_network',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [256, 128, 64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_7_shallow_network',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [64],\n",
    "        'entropy_coef': 0.01,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_8_high_entropy',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.05,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_9_no_entropy',\n",
    "        'learning_rate': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'entropy_coef': 0.0,\n",
    "        'num_episodes': 2000\n",
    "    },\n",
    "    {\n",
    "        'name': 'config_10_optimal',\n",
    "        'learning_rate': 0.002,\n",
    "        'gamma': 0.99,\n",
    "        'hidden_dims': [256, 128],\n",
    "        'entropy_coef': 0.02,\n",
    "        'num_episodes': 2000\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Total configurations to train: {len(HYPERPARAMETER_CONFIGS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Train All Configurations\n",
    "all_results = []\n",
    "\n",
    "for i, config in enumerate(HYPERPARAMETER_CONFIGS, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Configuration {i}/{len(HYPERPARAMETER_CONFIGS)}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Parameters: LR={config['learning_rate']}, Î³={config['gamma']}, \"\n",
    "          f\"Hidden={config['hidden_dims']}, Entropy={config['entropy_coef']}\")\n",
    "    \n",
    "    # Create environment and agent\n",
    "    env = FireRescueEnv(grid_size=10, max_time=200)\n",
    "    agent = REINFORCEAgent(\n",
    "        state_dim=env.obs_dim,\n",
    "        action_dim=env.action_space.n,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        gamma=config['gamma'],\n",
    "        hidden_dims=config['hidden_dims'],\n",
    "        entropy_coef=config['entropy_coef']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_path = f\"../models/reinforce/{config['name']}.pth\"\n",
    "    metrics = train_reinforce(\n",
    "        env, agent, \n",
    "        num_episodes=config['num_episodes'],\n",
    "        save_path=save_path,\n",
    "        config_name=config['name'],\n",
    "        verbose_freq=200\n",
    "    )\n",
    "    \n",
    "    # Add config to metrics\n",
    "    metrics['config'] = config\n",
    "    all_results.append(metrics)\n",
    "    \n",
    "    print(f\"âœ“ Best Avg Reward: {metrics['best_avg_reward']:.2f}\")\n",
    "    print(f\"âœ“ Avg Wall Collision Rate: {metrics['avg_wall_collision_rate']:.2%}\")\n",
    "    print(f\"âœ“ Avg Scan Efficiency: {metrics['avg_scan_efficiency']:.2%}\")\n",
    "    print(f\"âœ“ Avg Pickup Attempts: {metrics['avg_pickup_attempts']:.2f}\")\n",
    "    if metrics['avg_time_to_find_survivor']:\n",
    "        print(f\"âœ“ Avg Time to Find Survivor: {metrics['avg_time_to_find_survivor']:.1f} steps\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Save all results (convert numpy types to native Python types)\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "results_to_save = []\n",
    "for r in all_results:\n",
    "    result_copy = {k: v for k, v in r.items() if k != 'config'}\n",
    "    results_to_save.append(convert_to_json_serializable(result_copy))\n",
    "\n",
    "with open('../models/reinforce/all_results.json', 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85883c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Evaluation - Training Process Comparison\n",
    "def plot_training_comparison(results):\n",
    "    \"\"\"Plot comparison of all training runs.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Episode Rewards\n",
    "    ax = axes[0, 0]\n",
    "    for result in results:\n",
    "        rewards = result['episode_rewards']\n",
    "        smoothed = np.convolve(rewards, np.ones(50)/50, mode='valid')\n",
    "        ax.plot(smoothed, label=result['config_name'], alpha=0.7)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Smoothed Reward')\n",
    "    ax.set_title('Episode Rewards Across Configurations')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Wall Collision Rate\n",
    "    ax = axes[0, 1]\n",
    "    for result in results:\n",
    "        wall_collisions = result['wall_collision_rates']\n",
    "        smoothed = np.convolve(wall_collisions, np.ones(50)/50, mode='valid')\n",
    "        ax.plot(smoothed, label=result['config_name'], alpha=0.7)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Wall Collision Rate')\n",
    "    ax.set_title('Wall Collision Rate Across Configurations')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Scan Efficiency\n",
    "    ax = axes[1, 0]\n",
    "    for result in results:\n",
    "        scan_eff = result['scan_efficiencies']\n",
    "        smoothed = np.convolve(scan_eff, np.ones(50)/50, mode='valid')\n",
    "        ax.plot(smoothed, label=result['config_name'], alpha=0.7)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Scan Efficiency')\n",
    "    ax.set_title('Scan Efficiency Across Configurations')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Policy Loss\n",
    "    ax = axes[1, 1]\n",
    "    for result in results:\n",
    "        loss = result['loss_history']\n",
    "        smoothed = np.convolve(loss, np.ones(50)/50, mode='valid')\n",
    "        ax.plot(smoothed, label=result['config_name'], alpha=0.7)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Policy Loss')\n",
    "    ax.set_title('Policy Loss Across Configurations')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/reinforce/training_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_comparison(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Identify and Save Best Model\n",
    "# Find best configuration\n",
    "best_result = max(all_results, key=lambda x: x['best_avg_reward'])\n",
    "best_config_name = best_result['config_name']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BEST CONFIGURATION: {best_config_name}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Average Reward: {best_result['best_avg_reward']:.2f}\")\n",
    "print(f\"Avg Wall Collision Rate: {best_result['avg_wall_collision_rate']:.2%}\")\n",
    "print(f\"Avg Scan Efficiency: {best_result['avg_scan_efficiency']:.2%}\")\n",
    "print(f\"Avg Pickup Attempts: {best_result['avg_pickup_attempts']:.2f}\")\n",
    "if best_result['avg_time_to_find_survivor']:\n",
    "    print(f\"Avg Time to Find Survivor: {best_result['avg_time_to_find_survivor']:.1f} steps\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for key, value in best_result['config'].items():\n",
    "    if key != 'name':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Copy best model to best_model folder\n",
    "import shutil\n",
    "best_model_src = f\"../models/reinforce/{best_config_name}.pth\"\n",
    "best_model_dst = \"../models/reinforce/best_model/best_reinforce.pth\"\n",
    "shutil.copy(best_model_src, best_model_dst)\n",
    "\n",
    "# Save best config\n",
    "with open('../models/reinforce/best_model/config.json', 'w') as f:\n",
    "    json.dump(best_result['config'], f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Best model saved to: {best_model_dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe058a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: Evaluation Cell 2 - Best Model Performance\n",
    "def evaluate_best_model(model_path, config, num_episodes=100):\n",
    "    \"\"\"Evaluate the best model.\"\"\"\n",
    "    \n",
    "    env = FireRescueEnv(grid_size=10, max_time=250)\n",
    "    agent = REINFORCEAgent(\n",
    "        state_dim=env.obs_dim,\n",
    "        action_dim=env.action_space.n,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        gamma=config['gamma'],\n",
    "        hidden_dims=config['hidden_dims'],\n",
    "        entropy_coef=config['entropy_coef']\n",
    "    )\n",
    "    agent.load(model_path)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    successes = []\n",
    "    wall_collision_rates = []\n",
    "    scan_efficiencies = []\n",
    "    pickup_attempts_list = []\n",
    "    time_to_find_survivor_list = []\n",
    "    \n",
    "    print(\"Calculating episode performance metrics...\")\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # Extract metrics from final info\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(info.get('episode_length', 0))\n",
    "        successes.append(1 if info.get('success', False) else 0)\n",
    "        wall_collision_rates.append(info.get('wall_collision_rate', 0))\n",
    "        scan_efficiencies.append(info.get('scan_efficiency', 0))\n",
    "        pickup_attempts_list.append(info.get('pickup_attempts', 0))\n",
    "        \n",
    "        time_found = info.get('time_to_find_survivor', None)\n",
    "        if time_found is not None:\n",
    "            time_to_find_survivor_list.append(time_found)\n",
    "        \n",
    "        # Debug output for first 5 episodes\n",
    "        if episode < 5:\n",
    "            print(f\"  Episode {episode+1}: \"\n",
    "                  f\"Steps: {info.get('episode_length', 0)} | \"\n",
    "                  f\"Wall Collisions: {info.get('wall_collision_rate', 0):.2%} | \"\n",
    "                  f\"Scan Eff: {info.get('scan_efficiency', 0):.2%}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(episode_rewards),\n",
    "        'std_reward': np.std(episode_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "        'std_length': np.std(episode_lengths),\n",
    "        'success_rate': np.mean(successes),\n",
    "        'wall_collision_rate': np.mean(wall_collision_rates),\n",
    "        'scan_efficiency': np.mean(scan_efficiencies),\n",
    "        'avg_pickup_attempts': np.mean(pickup_attempts_list),\n",
    "        'avg_time_to_find_survivor': np.mean(time_to_find_survivor_list) if time_to_find_survivor_list else None,\n",
    "        'survivor_found_count': len(time_to_find_survivor_list),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'wall_collision_rates': wall_collision_rates,\n",
    "        'scan_efficiencies': scan_efficiencies\n",
    "    }\n",
    "\n",
    "# Evaluate best model\n",
    "eval_results = evaluate_best_model(\n",
    "    best_model_dst, \n",
    "    best_result['config'],\n",
    "    num_episodes=100\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Evaluation Summary:\")\n",
    "print(f\"   Average Episode Length: {eval_results['mean_length']:.1f} steps\")\n",
    "print(f\"   Wall Collision Rate: {eval_results['wall_collision_rate']:.2%}\")\n",
    "print(f\"   Scan Efficiency: {eval_results['scan_efficiency']:.2%}\")\n",
    "print(f\"   Avg Pickup Attempts: {eval_results['avg_pickup_attempts']:.2f}\")\n",
    "if eval_results['avg_time_to_find_survivor']:\n",
    "    print(f\"   Avg Time to Find Survivor: {eval_results['avg_time_to_find_survivor']:.1f} steps\")\n",
    "print(f\"   Success Rate: {eval_results['success_rate']:.2%}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST MODEL EVALUATION (100 Episodes)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Mean Reward: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}\")\n",
    "print(f\"Mean Episode Length: {eval_results['mean_length']:.1f} Â± {eval_results['std_length']:.1f}\")\n",
    "print(f\"Success Rate: {eval_results['success_rate']:.2%}\")\n",
    "print(f\"Wall Collision Rate: {eval_results['wall_collision_rate']:.2%}\")\n",
    "print(f\"Scan Efficiency: {eval_results['scan_efficiency']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Best Model Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Reward Distribution\n",
    "ax = axes[0, 0]\n",
    "ax.hist(eval_results['episode_rewards'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax.axvline(eval_results['mean_reward'], color='red', linestyle='--', \n",
    "           label=f\"Mean: {eval_results['mean_reward']:.2f}\")\n",
    "ax.set_xlabel('Episode Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Best Model: Reward Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Rewards Over Time\n",
    "ax = axes[0, 1]\n",
    "ax.plot(eval_results['episode_rewards'], alpha=0.6, label='Episode Reward')\n",
    "ax.axhline(eval_results['mean_reward'], color='red', linestyle='--', \n",
    "           label=f\"Mean: {eval_results['mean_reward']:.2f}\")\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Best Model: Episode Rewards')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Episode Length Distribution\n",
    "ax = axes[0, 2]\n",
    "ax.hist(eval_results['episode_lengths'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax.axvline(eval_results['mean_length'], color='red', linestyle='--',\n",
    "           label=f\"Mean: {eval_results['mean_length']:.1f}\")\n",
    "ax.set_xlabel('Episode Length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Best Model: Episode Length Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Wall Collision Rate Distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(eval_results['wall_collision_rates'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "ax.axvline(eval_results['wall_collision_rate'], color='darkred', linestyle='--',\n",
    "           label=f\"Mean: {eval_results['wall_collision_rate']:.2%}\")\n",
    "ax.set_xlabel('Wall Collision Rate')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Best Model: Wall Collision Rate')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Scan Efficiency Distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(eval_results['scan_efficiencies'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax.axvline(eval_results['scan_efficiency'], color='darkviolet', linestyle='--',\n",
    "           label=f\"Mean: {eval_results['scan_efficiency']:.2%}\")\n",
    "ax.set_xlabel('Scan Efficiency')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Best Model: Scan Efficiency')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Success Rate Summary\n",
    "ax = axes[1, 2]\n",
    "metrics = ['Success\\nRate', 'Wall\\nCollisions', 'Scan\\nEfficiency']\n",
    "values = [eval_results['success_rate'], eval_results['wall_collision_rate'], eval_results['scan_efficiency']]\n",
    "colors = ['green', 'red', 'purple']\n",
    "bars = ax.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Rate')\n",
    "ax.set_title('Best Model: Key Metrics Summary')\n",
    "ax.set_ylim([0, 1])\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/reinforce/best_model/evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ All plots saved successfully!\")\n",
    "print(\"âœ“ Training complete! Check '../models/reinforce/' for all saved models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
