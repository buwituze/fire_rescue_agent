{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e29c635",
   "metadata": {},
   "source": [
    "ACTOR-CRITIC (A2C) TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 1: IMPORTS AND SETUP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path for imports (for Jupyter notebook)\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from environment.custom_env import FireRescueEnv\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dedaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 2: A2C HYPERPARAMETER CONFIGURATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Key A2C Hyperparameters:\n",
    "- learning_rate: Step size for gradient updates\n",
    "- gamma: Discount factor (how much to value future rewards)\n",
    "- n_steps: Number of steps to run for each environment per update\n",
    "- ent_coef: Entropy coefficient (encourages exploration) - FIXED: Increased from 0.01 to 0.05\n",
    "- vf_coef: Value function coefficient\n",
    "- gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "- max_grad_norm: Maximum value for gradient clipping\n",
    "\n",
    "Note: Some configs are similar to DQN where applicable (lr, gamma, network arch)\n",
    "to enable cross-algorithm comparison.\n",
    "\n",
    "CRITICAL FIX: Entropy coefficients increased from 0.01 to 0.05 (matching PPO)\n",
    "to prevent action space collapse. Agent was only using movement actions.\n",
    "\"\"\"\n",
    "\n",
    "A2C_CONFIGS = {\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 1: BASELINE (Standard A2C settings)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_1_baseline\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Baseline - Standard A2C settings (FIXED exploration)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 2: HIGH LEARNING RATE (Similar to DQN config_2)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_2_high_lr\": {\n",
    "        \"learning_rate\": 5e-3,  # Very high\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"High LR - May cause instability (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 3: LOW LEARNING RATE (Similar to DQN config_3)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_3_low_lr\": {\n",
    "        \"learning_rate\": 1e-5,  # Very low\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Low LR - Slow but stable learning (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 4: LOW GAMMA (Similar to DQN config_6)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_4_low_gamma\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.95,  # Short-term focus\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Low Gamma - Myopic policy (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 5: HIGH ENTROPY (More exploration)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_5_high_entropy\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.1,  # High entropy = more exploration\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"High Entropy - Increased exploration\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 6: LOW ENTROPY (More exploitation)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_6_low_entropy\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.001,  # Low entropy = more exploitation\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Low Entropy - More exploitation\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 7: MANY STEPS (Longer rollouts)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_7_many_steps\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 20,  # Longer rollouts\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Many Steps - Longer rollouts (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 8: FEW STEPS (Shorter rollouts, more frequent updates)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_8_few_steps\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 2,  # Very short rollouts\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Few Steps - Frequent updates (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 9: HIGH VALUE FUNCTION WEIGHT\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_9_high_vf\": {\n",
    "        \"learning_rate\": 7e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 1.0,  # Higher value function weight\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"High VF Coef - Emphasizes value learning (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 10: DEEP NETWORK (Similar to DQN config_10)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_10_deep_net\": {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 10,\n",
    "        \"ent_coef\": 0.05,  # FIXED: Increased from 0.01\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[128, 128]),  # Deeper network\n",
    "        \"description\": \"Deep Network - More capacity (FIXED)\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 11: OPTIMIZED (Best combination for fire-rescue task)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_11_optimized\": {\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_steps\": 8,\n",
    "        \"ent_coef\": 0.06,  # FIXED: Increased from 0.02 for better exploration\n",
    "        \"vf_coef\": 0.6,\n",
    "        \"gae_lambda\": 0.98,  # Higher GAE lambda for smoother advantage\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[128, 64]),  # Asymmetric network\n",
    "        \"description\": \"Optimized - Balanced for task (FIXED)\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"\\nâœ“ Defined {len(A2C_CONFIGS)} A2C configurations (FIXED exploration)\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"{'Config':<25} {'LR':<10} {'Gamma':<8} {'Steps':<8} {'Ent':<8} {'VF':<8} {'GAE':<8} {'Network':<15}\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "for name, config in A2C_CONFIGS.items():\n",
    "    net_arch = str(config['policy_kwargs']['net_arch'])\n",
    "    print(f\"{name:<25} {config['learning_rate']:<10.0e} {config['gamma']:<8.3f} \"\n",
    "          f\"{config['n_steps']:<8} {config['ent_coef']:<8.3f} {config['vf_coef']:<8.2f} \"\n",
    "          f\"{config['gae_lambda']:<8.3f} {net_arch:<15}\")\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"\\nğŸ”§ CRITICAL FIXES APPLIED:\")\n",
    "print(\"  â€¢ Entropy coefficient increased from 0.01 to 0.05 (matching PPO)\")\n",
    "print(\"  â€¢ Prevents action space collapse (agent using all 6 actions)\")\n",
    "print(\"  â€¢ Encourages exploration of SCAN and PICKUP actions\")\n",
    "print(\"\\nCross-Algorithm Comparisons:\")\n",
    "print(\"  â€¢ Configs 2, 3: Similar to DQN - test learning rate extremes\")\n",
    "print(\"  â€¢ Config 4: Similar to DQN - test discount factor\")\n",
    "print(\"  â€¢ Config 10: Similar to DQN - deep network architecture\")\n",
    "print(\"\\nA2C-Specific Variations:\")\n",
    "print(\"  â€¢ Configs 5, 6: Entropy coefficient (exploration vs exploitation)\")\n",
    "print(\"  â€¢ Configs 7, 8: Rollout length (n_steps)\")\n",
    "print(\"  â€¢ Config 9: Value function emphasis\")\n",
    "print(\"  â€¢ Config 11: Optimized for fire-rescue task\")\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe61045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 3: TRAINING CALLBACK AND UTILITIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Callback to track training metrics - FIXED to match PPO approach\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_freq=1000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # FIXED: Use ep_info_buffer like PPO to get accurate episode info\n",
    "        if len(self.model.ep_info_buffer) > 0:\n",
    "            for info in self.model.ep_info_buffer:\n",
    "                if 'r' in info:\n",
    "                    self.episode_rewards.append(info['r'])\n",
    "                if 'l' in info:\n",
    "                    self.episode_lengths.append(info['l'])\n",
    "                self.episode_count += 1\n",
    "                \n",
    "        return True\n",
    "\n",
    "def evaluate_policy(model, env, n_episodes=100, verbose=False):\n",
    "    \"\"\"Evaluate trained policy with detailed metrics matching PPO notebook\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    wall_collision_rates = []\n",
    "    scan_efficiencies = []\n",
    "    pickup_attempts_list = []\n",
    "    time_to_find_survivor_list = []\n",
    "    \n",
    "    for episode_num in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # CRITICAL FIX: Extract metrics from final info (only available at episode end)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(info.get('episode_length', 0))\n",
    "        wall_collision_rates.append(info.get('wall_collision_rate', 0))\n",
    "        scan_efficiencies.append(info.get('scan_efficiency', 0))\n",
    "        pickup_attempts_list.append(info.get('pickup_attempts', 0))\n",
    "        \n",
    "        time_found = info.get('time_to_find_survivor', None)\n",
    "        if time_found is not None:\n",
    "            time_to_find_survivor_list.append(time_found)\n",
    "        \n",
    "        # Debug output for first 5 episodes\n",
    "        if verbose and episode_num < 5:\n",
    "            print(f\"  Episode {episode_num+1}: \"\n",
    "                  f\"Steps: {info.get('episode_length', 0)} | \"\n",
    "                  f\"Wall Collisions: {info.get('wall_collision_rate', 0):.2%} | \"\n",
    "                  f\"Scan Eff: {info.get('scan_efficiency', 0):.2%}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ“Š Evaluation Summary:\")\n",
    "        print(f\"   Average Episode Length: {np.mean(episode_lengths):.1f} steps\")\n",
    "        print(f\"   Wall Collision Rate: {np.mean(wall_collision_rates):.2%}\")\n",
    "        print(f\"   Scan Efficiency: {np.mean(scan_efficiencies):.2%}\")\n",
    "        print(f\"   Avg Pickup Attempts: {np.mean(pickup_attempts_list):.2f}\")\n",
    "        if time_to_find_survivor_list:\n",
    "            print(f\"   Avg Time to Find Survivor: {np.mean(time_to_find_survivor_list):.1f} steps ({len(time_to_find_survivor_list)}/{n_episodes} found)\")\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(episode_rewards),\n",
    "        'std_reward': np.std(episode_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "        'wall_collision_rate': np.mean(wall_collision_rates),\n",
    "        'scan_efficiency': np.mean(scan_efficiencies),\n",
    "        'avg_pickup_attempts': np.mean(pickup_attempts_list),\n",
    "        'avg_time_to_find_survivor': np.mean(time_to_find_survivor_list) if time_to_find_survivor_list else None,\n",
    "        'survivor_found_count': len(time_to_find_survivor_list),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'wall_collision_rates': wall_collision_rates,\n",
    "        'scan_efficiencies': scan_efficiencies,\n",
    "        'pickup_attempts': pickup_attempts_list,\n",
    "        'time_to_find_survivor': time_to_find_survivor_list,\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Training utilities defined (FIXED to match PPO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 4: TRAIN ALL CONFIGURATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "TOTAL_TIMESTEPS = 100_000\n",
    "EVAL_EPISODES = 50\n",
    "results = {}\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(f\"TRAINING ALL {len(A2C_CONFIGS)} CONFIGURATIONS\")\n",
    "print(f\"Total timesteps per config: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"{'='*120}\\n\")\n",
    "\n",
    "for config_name, config in A2C_CONFIGS.items():\n",
    "    print(f\"\\n{'â”€'*120}\")\n",
    "    print(f\"Training: {config_name}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"{'â”€'*120}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = FireRescueEnv(grid_size=10, max_time=250)\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    model_params = {k: v for k, v in config.items() if k not in ['description']}\n",
    "    \n",
    "    # Create model\n",
    "    model = A2C(\"MlpPolicy\", env, verbose=0, **model_params)\n",
    "    \n",
    "    # Create callback\n",
    "    callback = TrainingCallback()\n",
    "    \n",
    "    # Train\n",
    "    start_time = datetime.now()\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback, progress_bar=False)\n",
    "    train_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\nFinal evaluation ({EVAL_EPISODES} episodes)...\")\n",
    "    print(f\"Calculating episode performance metrics...\")\n",
    "    eval_results = evaluate_policy(model, env, n_episodes=EVAL_EPISODES, verbose=True)\n",
    "    \n",
    "    # Store results\n",
    "    results[config_name] = {\n",
    "        'config': config,\n",
    "        'callback': callback,\n",
    "        'eval_results': eval_results,\n",
    "        'train_time': train_time,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*120}\")\n",
    "    print(f\"RESULTS for {config_name}:\")\n",
    "    print(f\"   Mean Reward: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}\")\n",
    "    print(f\"   Avg Episode Length: {eval_results['mean_length']:.1f} steps\")\n",
    "    print(f\"   Wall Collision Rate: {eval_results['wall_collision_rate']:.2%}\")\n",
    "    print(f\"   Scan Efficiency: {eval_results['scan_efficiency']:.2%}\")\n",
    "    print(f\"   Avg Pickup Attempts: {eval_results['avg_pickup_attempts']:.2f}\")\n",
    "    if eval_results['avg_time_to_find_survivor'] is not None:\n",
    "        print(f\"   Avg Time to Find Survivor: {eval_results['avg_time_to_find_survivor']:.1f} steps\")\n",
    "    print(f\"   Training Time: {train_time:.1f}s ({train_time/60:.1f} min)\")\n",
    "    print(f\"{'='*120}\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # SAVE MODEL AND RESULTS IMMEDIATELY AFTER TRAINING\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(f\"\\nğŸ’¾ Saving {config_name} model and results...\")\n",
    "    \n",
    "    # Create directory for this configuration\n",
    "    config_dir = Path(f\"../models/a2c/{config_name}\")\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save(config_dir / f\"{config_name}_model\")\n",
    "    print(f\"   âœ“ Model saved: {config_dir}/{config_name}_model.zip\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_to_save = {k: v for k, v in config.items() if k != 'policy_kwargs'}\n",
    "    config_to_save['policy_kwargs'] = str(config['policy_kwargs'])\n",
    "    with open(config_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(config_to_save, f, indent=2)\n",
    "    print(f\"   âœ“ Config saved: {config_dir}/config.json\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results_to_save = {\n",
    "        'mean_reward': eval_results['mean_reward'],\n",
    "        'std_reward': eval_results['std_reward'],\n",
    "        'mean_length': eval_results['mean_length'],\n",
    "        'wall_collision_rate': eval_results['wall_collision_rate'],\n",
    "        'scan_efficiency': eval_results['scan_efficiency'],\n",
    "        'avg_pickup_attempts': eval_results['avg_pickup_attempts'],\n",
    "        'avg_time_to_find_survivor': eval_results['avg_time_to_find_survivor'],\n",
    "        'survivor_found_count': eval_results['survivor_found_count'],\n",
    "        'train_time': train_time,\n",
    "        'total_timesteps': TOTAL_TIMESTEPS,\n",
    "        'eval_episodes': EVAL_EPISODES,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(config_dir / \"results.json\", 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    print(f\"   âœ“ Results saved: {config_dir}/results.json\")\n",
    "    \n",
    "    # Save detailed episode data as numpy arrays\n",
    "    np.savez(\n",
    "        config_dir / \"episode_data.npz\",\n",
    "        episode_rewards=np.array(eval_results['episode_rewards']),\n",
    "        episode_lengths=np.array(eval_results['episode_lengths']),\n",
    "        wall_collision_rates=np.array(eval_results['wall_collision_rates']),\n",
    "        scan_efficiencies=np.array(eval_results['scan_efficiencies']),\n",
    "        pickup_attempts=np.array(eval_results['pickup_attempts']),\n",
    "        time_to_find_survivor=np.array(eval_results['time_to_find_survivor'])\n",
    "    )\n",
    "    print(f\"   âœ“ Episode data saved: {config_dir}/episode_data.npz\")\n",
    "    \n",
    "    print(f\"âœ… {config_name} fully saved!\\n\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"ALL TRAINING COMPLETED\")\n",
    "print(f\"{'='*120}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 5: OVERALL TRAINING VISUALIZATIONS (ALL CONFIGURATIONS)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FIXED: Now loads from saved results instead of requiring training run\n",
    "# This cell can be run independently without running Cell 4\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"LOADING SAVED RESULTS AND VISUALIZING\")\n",
    "print(f\"{'='*120}\\n\")\n",
    "\n",
    "# Get the correct path to models directory\n",
    "# Handle both cases: running from training/ or from project root\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Try to find the models directory\n",
    "if (current_dir / \"models\" / \"a2c\").exists():\n",
    "    models_dir = current_dir / \"models\" / \"a2c\"\n",
    "elif (current_dir.parent / \"models\" / \"a2c\").exists():\n",
    "    models_dir = current_dir.parent / \"models\" / \"a2c\"\n",
    "elif (current_dir / \"..\" / \"models\" / \"a2c\").exists():\n",
    "    models_dir = Path(current_dir / \"..\" / \"models\" / \"a2c\").resolve()\n",
    "else:\n",
    "    models_dir = Path(\"../models/a2c\")\n",
    "\n",
    "print(f\"Looking for models in: {models_dir}\")\n",
    "print(f\"Models directory exists: {models_dir.exists()}\\n\")\n",
    "\n",
    "loaded_results = {}\n",
    "\n",
    "# Check if directory exists\n",
    "if not models_dir.exists():\n",
    "    print(f\"âŒ ERROR: Directory {models_dir} does not exist!\")\n",
    "    print(\"Please run Cell 4 first to train the models.\")\n",
    "else:\n",
    "    # Get all config directories\n",
    "    config_dirs = [d for d in models_dir.iterdir() if d.is_dir() and d.name.startswith('config_')]\n",
    "    config_dirs.sort()\n",
    "    \n",
    "    print(f\"Found {len(config_dirs)} configurations:\\n\")\n",
    "    \n",
    "    for config_dir in config_dirs:\n",
    "        config_name = config_dir.name\n",
    "        results_file = config_dir / \"results.json\"\n",
    "        \n",
    "        if results_file.exists():\n",
    "            try:\n",
    "                with open(results_file, 'r') as f:\n",
    "                    result_data = json.load(f)\n",
    "                loaded_results[config_name] = result_data\n",
    "                print(f\"  âœ“ {config_name}: Mean Reward = {result_data['mean_reward']:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— {config_name}: Error loading results.json - {e}\")\n",
    "        else:\n",
    "            print(f\"  âœ— {config_name}: No results.json found\")\n",
    "    \n",
    "    print(f\"\\n{'='*120}\\n\")\n",
    "    \n",
    "    # Only create visualizations if we have data\n",
    "    if len(loaded_results) == 0:\n",
    "        print(\"âŒ No configuration results found!\")\n",
    "        print(\"Please run Cell 4 first to train the models.\")\n",
    "    else:\n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        fig.suptitle('A2C Training Results - All Configurations', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        config_names = sorted(loaded_results.keys())\n",
    "        colors = ['steelblue' for _ in config_names]\n",
    "        \n",
    "        # 1. Mean Evaluation Rewards (Bar Chart)\n",
    "        ax = axes[0, 0]\n",
    "        mean_rewards = [loaded_results[c]['mean_reward'] for c in config_names]\n",
    "        \n",
    "        bars = ax.barh(range(len(config_names)), mean_rewards, color=colors, alpha=0.7)\n",
    "        ax.set_yticks(range(len(config_names)))\n",
    "        ax.set_yticklabels([c.replace('config_', '').replace('_', ' ').title() \n",
    "                             for c in config_names], fontsize=8)\n",
    "        ax.set_xlabel('Mean Reward', fontweight='bold')\n",
    "        ax.set_title('Evaluation Performance Comparison', fontweight='bold')\n",
    "        ax.axvline(x=0, color='red', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, mean_rewards)):\n",
    "            ax.text(val, i, f' {val:.1f}', va='center', fontsize=7)\n",
    "        \n",
    "        # 2. Scan Efficiency (Bar Chart)\n",
    "        ax = axes[0, 1]\n",
    "        scan_efficiencies = [loaded_results[c]['scan_efficiency'] * 100 for c in config_names]\n",
    "        bars = ax.barh(range(len(config_names)), scan_efficiencies, color='green', alpha=0.7)\n",
    "        ax.set_yticks(range(len(config_names)))\n",
    "        ax.set_yticklabels([c.replace('config_', '').replace('_', ' ').title() \n",
    "                             for c in config_names], fontsize=8)\n",
    "        ax.set_xlabel('Scan Efficiency (%)', fontweight='bold')\n",
    "        ax.set_title('Scan Efficiency Comparison', fontweight='bold')\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, scan_efficiencies)):\n",
    "            ax.text(val, i, f' {val:.1f}%', va='center', fontsize=7)\n",
    "        \n",
    "        # 3. Survivor Found Rate (Bar Chart) - REPLACED training curves\n",
    "        ax = axes[0, 2]\n",
    "        survivor_found_pct = [(loaded_results[c]['survivor_found_count'] / loaded_results[c]['eval_episodes'] * 100) \n",
    "                              for c in config_names]\n",
    "        bars = ax.barh(range(len(config_names)), survivor_found_pct, color='purple', alpha=0.7)\n",
    "        ax.set_yticks(range(len(config_names)))\n",
    "        ax.set_yticklabels([c.replace('config_', '').replace('_', ' ').title() \n",
    "                             for c in config_names], fontsize=8)\n",
    "        ax.set_xlabel('Survivor Found Rate (%)', fontweight='bold')\n",
    "        ax.set_title('Success Rate Comparison', fontweight='bold')\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, survivor_found_pct)):\n",
    "            ax.text(val, i, f' {val:.1f}%', va='center', fontsize=7)\n",
    "        \n",
    "        # 4. Training Time Comparison\n",
    "        ax = axes[1, 0]\n",
    "        train_times = [loaded_results[c]['train_time'] for c in config_names]\n",
    "        bars = ax.barh(range(len(config_names)), train_times, color=colors, alpha=0.7)\n",
    "        ax.set_yticks(range(len(config_names)))\n",
    "        ax.set_yticklabels([c.replace('config_', '').replace('_', ' ').title() \n",
    "                             for c in config_names], fontsize=8)\n",
    "        ax.set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "        ax.set_title('Training Time Comparison', fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, train_times)):\n",
    "            ax.text(val, i, f' {val:.0f}s', va='center', fontsize=7)\n",
    "        \n",
    "        # 5. Wall Collision Rate vs Scan Efficiency Scatter\n",
    "        ax = axes[1, 1]\n",
    "        for i, config_name in enumerate(config_names):\n",
    "            wall_collision = loaded_results[config_name]['wall_collision_rate'] * 100\n",
    "            scan_eff = loaded_results[config_name]['scan_efficiency'] * 100\n",
    "            reward = loaded_results[config_name]['mean_reward']\n",
    "            \n",
    "            # Color by reward performance\n",
    "            color = 'green' if reward > 50 else 'orange' if reward > 0 else 'red'\n",
    "            ax.scatter(wall_collision, scan_eff, color=color, s=100, marker='o', \n",
    "                      alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "        \n",
    "        ax.set_xlabel('Wall Collision Rate (%)', fontweight='bold')\n",
    "        ax.set_ylabel('Scan Efficiency (%)', fontweight='bold')\n",
    "        ax.set_title('Wall Collisions vs Scan Efficiency', fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Add legend for colors\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor='green', alpha=0.7, label='Good (>50)'),\n",
    "                           Patch(facecolor='orange', alpha=0.7, label='Medium (0-50)'),\n",
    "                           Patch(facecolor='red', alpha=0.7, label='Poor (<0)')]\n",
    "        ax.legend(handles=legend_elements, loc='best', fontsize=8)\n",
    "        \n",
    "        # 6. Episode Length Comparison\n",
    "        ax = axes[1, 2]\n",
    "        mean_lengths = [loaded_results[c]['mean_length'] for c in config_names]\n",
    "        bars = ax.barh(range(len(config_names)), mean_lengths, color='coral', alpha=0.7)\n",
    "        ax.set_yticks(range(len(config_names)))\n",
    "        ax.set_yticklabels([c.replace('config_', '').replace('_', ' ').title() \n",
    "                             for c in config_names], fontsize=8)\n",
    "        ax.set_xlabel('Mean Episode Length', fontweight='bold')\n",
    "        ax.set_title('Episode Length Comparison', fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, mean_lengths)):\n",
    "            ax.text(val, i, f' {val:.0f}', va='center', fontsize=7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Overall training visualizations complete\\n\")\n",
    "        print(\"Note: This cell now loads from saved results and can run independently!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 6: IDENTIFY BEST MODEL FROM SAVED RESULTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FIXED: Now loads from saved results instead of requiring training run\n",
    "# This cell can be run independently without running Cell 4\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"IDENTIFYING BEST CONFIGURATION FROM SAVED RESULTS\")\n",
    "print(f\"{'='*120}\\n\")\n",
    "\n",
    "# Load all saved results from disk (if not already loaded from Cell 5)\n",
    "if 'loaded_results' not in locals() or not loaded_results:\n",
    "    # Get the correct path to models directory\n",
    "    current_dir = Path.cwd()\n",
    "    if (current_dir / \"models\" / \"a2c\").exists():\n",
    "        models_dir = current_dir / \"models\" / \"a2c\"\n",
    "    elif (current_dir.parent / \"models\" / \"a2c\").exists():\n",
    "        models_dir = current_dir.parent / \"models\" / \"a2c\"\n",
    "    else:\n",
    "        models_dir = Path(\"../models/a2c\")\n",
    "    \n",
    "    loaded_results = {}\n",
    "    config_dirs = [d for d in models_dir.iterdir() if d.is_dir() and d.name.startswith('config_')]\n",
    "    \n",
    "    for config_dir in config_dirs:\n",
    "        config_name = config_dir.name\n",
    "        results_file = config_dir / \"results.json\"\n",
    "        config_file = config_dir / \"config.json\"\n",
    "        \n",
    "        if results_file.exists() and config_file.exists():\n",
    "            with open(results_file, 'r') as f:\n",
    "                result_data = json.load(f)\n",
    "            with open(config_file, 'r') as f:\n",
    "                config_data = json.load(f)\n",
    "            \n",
    "            # Store both results and config\n",
    "            loaded_results[config_name] = {\n",
    "                'results': result_data,\n",
    "                'config': config_data\n",
    "            }\n",
    "\n",
    "# Check if we have data\n",
    "if not loaded_results:\n",
    "    print(\"âŒ ERROR: No configuration results found!\")\n",
    "    print(\"Please run Cell 4 first to train the models.\")\n",
    "else:\n",
    "    # Find best configuration based on evaluation reward\n",
    "    # Handle both data structures (direct results or nested with 'results' key)\n",
    "    if 'results' in list(loaded_results.values())[0]:\n",
    "        # Nested structure\n",
    "        best_config_name = max(loaded_results.keys(), \n",
    "                               key=lambda k: loaded_results[k]['results']['mean_reward'])\n",
    "        best_result = loaded_results[best_config_name]['results']\n",
    "        best_config = loaded_results[best_config_name]['config']\n",
    "    else:\n",
    "        # Direct structure (from Cell 5)\n",
    "        best_config_name = max(loaded_results.keys(), \n",
    "                               key=lambda k: loaded_results[k]['mean_reward'])\n",
    "        best_result = loaded_results[best_config_name]\n",
    "        # Load config separately\n",
    "        config_file = Path(\"../models/a2c\") / best_config_name / \"config.json\"\n",
    "        if config_file.exists():\n",
    "            with open(config_file, 'r') as f:\n",
    "                best_config = json.load(f)\n",
    "        else:\n",
    "            best_config = {'description': 'Config file not found'}\n",
    "    \n",
    "    print(f\"BEST PERFORMING CONFIGURATION: {best_config_name}\")\n",
    "    print(f\"{'='*120}\")\n",
    "    print(f\"Description: {best_config.get('description', 'N/A')}\")\n",
    "    print(f\"\\nHyperparameters:\")\n",
    "    for key, value in best_config.items():\n",
    "        if key != 'description':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Mean Reward: {best_result['mean_reward']:.2f} Â± {best_result['std_reward']:.2f}\")\n",
    "    survivor_rate = (best_result['survivor_found_count'] / best_result['eval_episodes']) * 100\n",
    "    print(f\"  Survivor Found Rate: {survivor_rate:.1f}% ({best_result['survivor_found_count']}/{best_result['eval_episodes']})\")\n",
    "    print(f\"  Mean Episode Length: {best_result['mean_length']:.1f}\")\n",
    "    print(f\"  Wall Collision Rate: {best_result['wall_collision_rate']:.2%}\")\n",
    "    print(f\"  Scan Efficiency: {best_result['scan_efficiency']:.2%}\")\n",
    "    if best_result.get('avg_time_to_find_survivor') is not None:\n",
    "        print(f\"  Avg Time to Find Survivor: {best_result['avg_time_to_find_survivor']:.1f} steps\")\n",
    "    print(f\"  Training Time: {best_result['train_time']:.1f}s ({best_result['train_time']/60:.1f} min)\")\n",
    "    print(f\"{'='*120}\\n\")\n",
    "    \n",
    "    print(f\"Best model location: ../models/a2c/{best_config_name}/\")\n",
    "    print(f\"  - Model: {best_config_name}_model.zip\")\n",
    "    print(f\"  - Config: config.json\")\n",
    "    print(f\"  - Results: results.json\")\n",
    "    print(f\"  - Episode Data: episode_data.npz\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 7: VISUALIZE BEST MODEL FROM SAVED DATA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FIXED: Now loads episode data from saved files instead of re-running evaluation\n",
    "# This cell can be run independently without running Cell 4 or re-evaluating\n",
    "\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"VISUALIZING BEST MODEL PERFORMANCE FROM SAVED DATA\")\n",
    "print(f\"{'='*120}\\n\")\n",
    "\n",
    "# Load best config info (if not already loaded from Cell 6)\n",
    "if 'best_config_name' not in locals():\n",
    "    models_dir = Path(\"../models/a2c\")\n",
    "    loaded_results = {}\n",
    "    \n",
    "    config_dirs = [d for d in models_dir.iterdir() if d.is_dir() and d.name.startswith('config_')]\n",
    "    \n",
    "    for config_dir in config_dirs:\n",
    "        config_name = config_dir.name\n",
    "        results_file = config_dir / \"results.json\"\n",
    "        \n",
    "        if results_file.exists():\n",
    "            with open(results_file, 'r') as f:\n",
    "                result_data = json.load(f)\n",
    "            loaded_results[config_name] = result_data\n",
    "    \n",
    "    best_config_name = max(loaded_results.keys(), \n",
    "                           key=lambda k: loaded_results[k]['mean_reward'])\n",
    "\n",
    "# Load episode data from saved npz file\n",
    "episode_data_path = Path(f\"../models/a2c/{best_config_name}/episode_data.npz\")\n",
    "results_path = Path(f\"../models/a2c/{best_config_name}/results.json\")\n",
    "\n",
    "if not episode_data_path.exists():\n",
    "    print(f\"âŒ ERROR: Episode data not found at {episode_data_path}\")\n",
    "    print(\"Please run Cell 4 first to train and save the models.\")\n",
    "else:\n",
    "    # Load the data\n",
    "    data = np.load(episode_data_path)\n",
    "    episode_rewards = data['episode_rewards']\n",
    "    episode_lengths = data['episode_lengths']\n",
    "    wall_collision_rates = data['wall_collision_rates']\n",
    "    scan_efficiencies = data['scan_efficiencies']\n",
    "    pickup_attempts = data['pickup_attempts']\n",
    "    time_to_find_survivor = data['time_to_find_survivor']\n",
    "    \n",
    "    with open(results_path, 'r') as f:\n",
    "        results_summary = json.load(f)\n",
    "    \n",
    "    n_eval_episodes = len(episode_rewards)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Configuration: {best_config_name}\")\n",
    "    print(f\"Episodes: {n_eval_episodes}\")\n",
    "    print(f\"Mean Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Min/Max Reward: {np.min(episode_rewards):.2f} / {np.max(episode_rewards):.2f}\")\n",
    "    print(f\"Mean Episode Length: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}\")\n",
    "    print(f\"Wall Collision Rate: {np.mean(wall_collision_rates):.2%}\")\n",
    "    print(f\"Scan Efficiency: {np.mean(scan_efficiencies):.2%}\")\n",
    "    print(f\"Avg Pickup Attempts: {np.mean(pickup_attempts):.2f}\")\n",
    "    if len(time_to_find_survivor) > 0:\n",
    "        print(f\"Survivors Found: {len(time_to_find_survivor)}/{n_eval_episodes}\")\n",
    "        print(f\"Avg Time to Find Survivor: {np.mean(time_to_find_survivor):.1f} steps\")\n",
    "    \n",
    "    # Create evaluation visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'Best Model Performance - {best_config_name}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Reward Distribution\n",
    "    axes[0].hist(episode_rewards, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(np.mean(episode_rewards), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {np.mean(episode_rewards):.1f}')\n",
    "    axes[0].set_xlabel('Episode Reward', fontweight='bold')\n",
    "    axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0].set_title('Reward Distribution', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Scan Efficiency distribution\n",
    "    axes[1].hist(scan_efficiencies*100, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(np.mean(scan_efficiencies)*100, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {np.mean(scan_efficiencies)*100:.1f}%')\n",
    "    axes[1].set_xlabel('Scan Efficiency (%)', fontweight='bold')\n",
    "    axes[1].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[1].set_title('Scan Efficiency Distribution', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Wall Collision Rate distribution\n",
    "    axes[2].hist(wall_collision_rates*100, bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[2].axvline(np.mean(wall_collision_rates)*100, color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {np.mean(wall_collision_rates)*100:.1f}%')\n",
    "    axes[2].set_xlabel('Wall Collision Rate (%)', fontweight='bold')\n",
    "    axes[2].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[2].set_title('Wall Collision Rate Distribution', fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional metrics visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle(f'Additional Metrics - {best_config_name}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. Episode Length Distribution\n",
    "    axes[0].hist(episode_lengths, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(np.mean(episode_lengths), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {np.mean(episode_lengths):.1f}')\n",
    "    axes[0].set_xlabel('Episode Length (steps)', fontweight='bold')\n",
    "    axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0].set_title('Episode Length Distribution', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Time to Find Survivor (for successful episodes)\n",
    "    if len(time_to_find_survivor) > 0:\n",
    "        axes[1].hist(time_to_find_survivor, bins=20, color='gold', alpha=0.7, edgecolor='black')\n",
    "        axes[1].axvline(np.mean(time_to_find_survivor), color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {np.mean(time_to_find_survivor):.1f}')\n",
    "        axes[1].set_xlabel('Time to Find Survivor (steps)', fontweight='bold')\n",
    "        axes[1].set_ylabel('Frequency', fontweight='bold')\n",
    "        axes[1].set_title(f'Time to Find Survivor (n={len(time_to_find_survivor)})', fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No survivors found\\nin evaluation episodes', \n",
    "                    ha='center', va='center', fontsize=12, transform=axes[1].transAxes)\n",
    "        axes[1].set_title('Time to Find Survivor', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Visualization completed\")\n",
    "    print(f\"{'='*120}\")\n",
    "    print(\"\\nNote: This cell loads from saved episode data.\")\n",
    "    print(\"To re-evaluate the model with new episodes, load and run the model manually.\")\n",
    "    print(f\"{'='*120}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
