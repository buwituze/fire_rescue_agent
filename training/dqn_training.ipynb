{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cb3579",
   "metadata": {},
   "source": [
    "# DQN Training for Fire-Rescue Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08b0a5",
   "metadata": {},
   "source": [
    "#### Deep Q-Network - Value-Based Reinforcement Learning\n",
    " \n",
    "This notebook includes:\n",
    " - 10+ hyperparameter configurations\n",
    " - Training with different settings\n",
    " - Logging and visualization\n",
    " - Model saving and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium stable-baselines3 tensorboard matplotlib pandas seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 2: Import Libraries\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Stable Baselines\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'training' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import custom environment\n",
    "from environment.custom_env import FireRescueEnv\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "print(f\"âœ“ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0271b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 3: Setup Directories\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Create necessary directories for models and logs\n",
    "\"\"\"\n",
    "# Create directory structure\n",
    "base_dir = project_root / \"models\" / \"dqn\"\n",
    "logs_dir = base_dir / \"training_logs\"\n",
    "tensorboard_dir = base_dir / \"tensorboard\"\n",
    "results_dir = project_root / \"results\"\n",
    "plots_dir = results_dir / \"plots\"\n",
    "\n",
    "for directory in [base_dir, logs_dir, tensorboard_dir, results_dir, plots_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Directories created:\")\n",
    "print(f\"  - Models: {base_dir}\")\n",
    "print(f\"  - Logs: {logs_dir}\")\n",
    "print(f\"  - Tensorboard: {tensorboard_dir}\")\n",
    "print(f\"  - Results: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb61a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# cell 4: DQN HYPERPARAMETER CONFIGURATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Focused on key hyperparameters mentioned by instructor:\n",
    "- learning_rate (lr)\n",
    "- gamma (discount factor)\n",
    "- batch_size\n",
    "- epsilon_start (exploration_initial_eps)\n",
    "- epsilon_end (exploration_final_eps)\n",
    "- epsilon_decay (exploration_fraction)\n",
    "\n",
    "Standardized across all configs:\n",
    "- buffer_size = 50000 (same for all)\n",
    "- learning_starts = 1000 (same for all)\n",
    "- tau = 1.0 (hard target network updates)\n",
    "\n",
    "Network architecture (policy_kwargs):\n",
    "- Most use [64, 64] (standard)\n",
    "- Some use [128, 128] (deeper) or [32, 32] (shallower)\n",
    "\"\"\"\n",
    "\n",
    "DQN_CONFIGS = {\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 1: BASELINE (Good starting point)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_1_baseline\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized parameters\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Baseline - Standard settings\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 2: HIGH LEARNING RATE (Too aggressive - may not converge)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_2_high_lr\": {\n",
    "        \"learning_rate\": 5e-3,  # Very high\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"High LR - Unstable learning\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 3: LOW LEARNING RATE (Too slow - won't learn much)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_3_low_lr\": {\n",
    "        \"learning_rate\": 1e-5,  # Very low\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Low LR - Slow learning\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 4: LARGE BATCH (More stable but slower)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_4_large_batch\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 128,  # Large batch\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Large Batch - Stable gradients\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 5: SMALL BATCH (Noisier but faster)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_5_small_batch\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 16,  # Small batch\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Small Batch - Fast updates\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 6: LOW GAMMA (Short-sighted - doesn't plan ahead)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_6_low_gamma\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.95,  # Low discount factor\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Low Gamma - Myopic policy\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 7: HIGH GAMMA (Very forward-looking)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_7_high_gamma\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.995,  # High discount factor\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.05,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"High Gamma - Long-term planning\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 8: FAST EXPLORATION DECAY (Stops exploring too soon)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_8_fast_decay\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.1,  # Higher final epsilon\n",
    "        \"exploration_fraction\": 0.1,  # Fast decay\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Fast Decay - Premature exploitation\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 9: SLOW EXPLORATION DECAY (Explores longer)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_9_slow_decay\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.01,  # Very low final epsilon\n",
    "        \"exploration_fraction\": 0.4,  # Slow decay\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Slow Decay - Extended exploration\"\n",
    "    },\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 10: OPTIMIZED (Best guess based on task)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_10_optimized\": {\n",
    "        \"learning_rate\": 3e-4,  # Sweet spot\n",
    "        \"gamma\": 0.99,\n",
    "        \"batch_size\": 64,  # Good balance\n",
    "        \"exploration_initial_eps\": 1.0,\n",
    "        \"exploration_final_eps\": 0.02,\n",
    "        \"exploration_fraction\": 0.3,  # Balanced exploration\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[128, 128]),  # Deeper network\n",
    "        \"description\": \"Optimized - Well-balanced settings\"\n",
    "    },\n",
    "    \n",
    "     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Config 11: Zero epsilon start and end\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \"config_11_low_gamma\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"gamma\": 0.95,\n",
    "        \"batch_size\": 32,\n",
    "        \"exploration_initial_eps\": 0,\n",
    "        \"exploration_final_eps\": 0,\n",
    "        \"exploration_fraction\": 0.2,\n",
    "        # Standardized\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_starts\": 1000,\n",
    "        \"tau\": 1.0,\n",
    "        \"policy_kwargs\": dict(net_arch=[64, 64]),\n",
    "        \"description\": \"Zero Epsilon - No exploration\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION SUMMARY TABLE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(f\"âœ“ Defined {len(DQN_CONFIGS)} DQN configurations\\n\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Config':<25} {'LR':<10} {'Gamma':<8} {'Batch':<8} {'Îµ_start':<10} {'Îµ_end':<10} {'Îµ_decay':<10} {'Network':<15}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for config_name, config in DQN_CONFIGS.items():\n",
    "    net_arch = str(config['policy_kwargs']['net_arch'])\n",
    "    print(f\"{config_name:<25} {config['learning_rate']:<10.0e} {config['gamma']:<8.3f} \"\n",
    "          f\"{config['batch_size']:<8} {config['exploration_initial_eps']:<10.2f} \"\n",
    "          f\"{config['exploration_final_eps']:<10.2f} {config['exploration_fraction']:<10.2f} {net_arch:<15}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  â€¢ Configs 2 & 3: Test extreme learning rates\")\n",
    "print(\"  â€¢ Configs 4 & 5: Test batch size impact\")\n",
    "print(\"  â€¢ Configs 6 & 7: Test discount factor (short vs long-term)\")\n",
    "print(\"  â€¢ Configs 8 & 9: Test exploration strategy\")\n",
    "print(\"  â€¢ Config 10: Optimized combination\")\n",
    "print(\"\\nAll configs use:\")\n",
    "print(\"  â€¢ buffer_size = 50,000 (standardized)\")\n",
    "print(\"  â€¢ learning_starts = 1,000 (standardized)\")\n",
    "print(\"  â€¢ tau = 1.0 (hard target updates)\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 5: Training Configuration\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Set training parameters\n",
    "\"\"\"\n",
    "# Training settings\n",
    "TOTAL_TIMESTEPS = 150000  # Total training steps per configuration\n",
    "EVAL_FREQ = 5000          # Evaluate every N steps\n",
    "N_EVAL_EPISODES = 10      # Number of episodes for evaluation\n",
    "SAVE_FREQ = 10000         # Save model checkpoint every N steps\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  - Total Timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"  - Evaluation Frequency: {EVAL_FREQ:,}\")\n",
    "print(f\"  - Evaluation Episodes: {N_EVAL_EPISODES}\")\n",
    "print(f\"  - Checkpoint Frequency: {SAVE_FREQ:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109961ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 6: Training Function\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Function to train a single DQN configuration\n",
    "\"\"\"\n",
    "\n",
    "def train_dqn_config(config_name, config, total_timesteps=TOTAL_TIMESTEPS):\n",
    "    \"\"\"\n",
    "    Train DQN with given configuration\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of the configuration\n",
    "        config: Dictionary of hyperparameters\n",
    "        total_timesteps: Total training steps\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {config_name}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = FireRescueEnv(grid_size=10, max_time=180)\n",
    "    env = Monitor(env)\n",
    "    \n",
    "    # Create eval environment\n",
    "    eval_env = FireRescueEnv(grid_size=10, max_time=180)\n",
    "    eval_env = Monitor(eval_env)\n",
    "    \n",
    "    # Create model directory for this config\n",
    "    model_dir = base_dir / config_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Setup callbacks\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=str(model_dir),\n",
    "        log_path=str(logs_dir / config_name),\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        n_eval_episodes=N_EVAL_EPISODES,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=SAVE_FREQ,\n",
    "        save_path=str(model_dir / \"checkpoints\"),\n",
    "        name_prefix=\"dqn_checkpoint\",\n",
    "        save_replay_buffer=True,\n",
    "        save_vecnormalize=True\n",
    "    )\n",
    "    \n",
    "    callback_list = CallbackList([eval_callback, checkpoint_callback])\n",
    "    \n",
    "    # Extract hyperparameters (remove description)\n",
    "    train_config = {k: v for k, v in config.items() if k != 'description'}\n",
    "    \n",
    "    # Create DQN model\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        seed=SEED,\n",
    "        tensorboard_log=str(tensorboard_dir / config_name),\n",
    "        **train_config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    start_time = datetime.now()\n",
    "    print(f\"\\nStarting training at {start_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback_list,\n",
    "            progress_bar=True\n",
    "        )\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\nâœ“ Training completed in {training_time:.1f} seconds\")\n",
    "        \n",
    "        # Save final model\n",
    "        final_model_path = model_dir / \"final_model\"\n",
    "        model.save(str(final_model_path))\n",
    "        print(f\"âœ“ Model saved to {final_model_path}\")\n",
    "        \n",
    "        # Evaluate final model\n",
    "        print(\"\\nEvaluating final model...\")\n",
    "        mean_reward, std_reward = evaluate_policy(\n",
    "            model, \n",
    "            eval_env, \n",
    "            n_eval_episodes=20,\n",
    "            deterministic=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Mean Reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "        \n",
    "        # Calculate success rate\n",
    "        success_count = 0\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        \n",
    "        for _ in range(20):\n",
    "            obs, info = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, _states = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(episode_length)\n",
    "            \n",
    "            if info.get('success', False):\n",
    "                success_count += 1\n",
    "        \n",
    "        success_rate = success_count / 20\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'config_name': config_name,\n",
    "            'description': config['description'],\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'success_rate': success_rate,\n",
    "            'mean_episode_length': np.mean(episode_lengths),\n",
    "            'std_episode_length': np.std(episode_lengths),\n",
    "            'training_time': training_time,\n",
    "            'total_timesteps': total_timesteps,\n",
    "            'hyperparameters': train_config\n",
    "        }\n",
    "        \n",
    "        # Save results to JSON\n",
    "        results_file = model_dir / \"results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            # Convert numpy types to Python types for JSON\n",
    "            json_results = {k: (float(v) if isinstance(v, (np.floating, np.integer)) else v) \n",
    "                          for k, v in results.items() if k != 'hyperparameters'}\n",
    "            json_results['hyperparameters'] = {\n",
    "                k: (float(v) if isinstance(v, (np.floating, np.integer)) else v)\n",
    "                for k, v in results['hyperparameters'].items() if k != 'policy_kwargs'\n",
    "            }\n",
    "            json.dump(json_results, f, indent=4)\n",
    "        \n",
    "        print(f\"âœ“ Results saved to {results_file}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Training failed: {e}\")\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "        return None\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2aad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 7: Train All Configurations\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Train all DQN configurations\n",
    "WARNING: This will take several hours!\n",
    "\"\"\"\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"STARTING FULL TRAINING RUN\")\n",
    "print(f\"Training {len(DQN_CONFIGS)} configurations\")\n",
    "print(f\"Estimated time: {len(DQN_CONFIGS) * 15} - {len(DQN_CONFIGS) * 30} minutes\")\n",
    "print(f\"{'#'*70}\\n\")\n",
    "\n",
    "# Train each configuration\n",
    "for config_name, config in DQN_CONFIGS.items():\n",
    "    results = train_dqn_config(config_name, config)\n",
    "    \n",
    "    if results:\n",
    "        all_results.append(results)\n",
    "        print(f\"\\nâœ“ Completed {len(all_results)}/{len(DQN_CONFIGS)} configurations\")\n",
    "    else:\n",
    "        print(f\"\\nâœ— Failed configuration: {config_name}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"TRAINING COMPLETE!\")\n",
    "print(f\"Successfully trained: {len(all_results)}/{len(DQN_CONFIGS)} configurations\")\n",
    "print(f\"{'#'*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55289476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 8: Create Results DataFrame\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Organize results into a pandas DataFrame for analysis\n",
    "\"\"\"\n",
    "\n",
    "# Check if we have any results\n",
    "if len(all_results) == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NO SUCCESSFUL TRAINING RUNS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. Missing packages: Run 'pip install stable-baselines3[extra]'\")\n",
    "    print(\"2. Environment import error: Check environment/custom_env.py exists\")\n",
    "    print(\"3. GPU/memory issues: Try reducing TOTAL_TIMESTEPS in Cell 5\")\n",
    "    print(\"\\nPlease fix the errors above and re-run Cell 7\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DQN TRAINING RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(results_df[['config_name', 'mean_reward', 'std_reward', 'success_rate', 'training_time']].to_string(index=False))\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_path = results_dir / \"dqn_results.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nâœ“ Results saved to {csv_path}\")\n",
    "\n",
    "    # Find best configuration\n",
    "    best_config = results_df.loc[results_df['mean_reward'].idxmax()]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"BEST CONFIGURATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Name: {best_config['config_name']}\")\n",
    "    print(f\"Description: {best_config['description']}\")\n",
    "    print(f\"Mean Reward: {best_config['mean_reward']:.2f} Â± {best_config['std_reward']:.2f}\")\n",
    "    print(f\"Success Rate: {best_config['success_rate']*100:.1f}%\")\n",
    "    print(f\"Training Time: {best_config['training_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522bdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 9: Visualize Results - Comparison Plot\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Create comprehensive visualization of results\n",
    "\"\"\"\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Sort by mean reward\n",
    "results_sorted = results_df.sort_values('mean_reward', ascending=False)\n",
    "\n",
    "# 1. Mean Reward Comparison\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.barh(range(len(results_sorted)), results_sorted['mean_reward'], \n",
    "                xerr=results_sorted['std_reward'], capsize=5)\n",
    "ax1.set_yticks(range(len(results_sorted)))\n",
    "ax1.set_yticklabels(results_sorted['config_name'], fontsize=9)\n",
    "ax1.set_xlabel('Mean Reward', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('DQN Configuration Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Color bars by performance\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(bars)))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Success Rate\n",
    "ax2 = axes[0, 1]\n",
    "success_sorted = results_df.sort_values('success_rate', ascending=False)\n",
    "bars2 = ax2.barh(range(len(success_sorted)), success_sorted['success_rate'] * 100)\n",
    "ax2.set_yticks(range(len(success_sorted)))\n",
    "ax2.set_yticklabels(success_sorted['config_name'], fontsize=9)\n",
    "ax2.set_xlabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Mission Success Rate by Configuration', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Color bars\n",
    "colors2 = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(bars2)))\n",
    "for bar, color in zip(bars2, colors2):\n",
    "    bar.set_color(color)\n",
    "\n",
    "# 3. Training Time vs Performance\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(results_df['training_time'], results_df['mean_reward'], \n",
    "                     s=results_df['success_rate']*500, alpha=0.6, \n",
    "                     c=results_df['mean_reward'], cmap='RdYlGn')\n",
    "ax3.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Mean Reward', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Training Efficiency: Time vs Performance', fontsize=14, fontweight='bold')\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Add labels for best and worst\n",
    "best_idx = results_df['mean_reward'].idxmax()\n",
    "worst_idx = results_df['mean_reward'].idxmin()\n",
    "ax3.annotate('Best', xy=(results_df.loc[best_idx, 'training_time'], \n",
    "                         results_df.loc[best_idx, 'mean_reward']),\n",
    "            xytext=(10, 10), textcoords='offset points',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='green', alpha=0.3),\n",
    "            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# Colorbar\n",
    "plt.colorbar(scatter, ax=ax3, label='Mean Reward')\n",
    "\n",
    "# 4. Hyperparameter Impact - Learning Rate\n",
    "ax4 = axes[1, 1]\n",
    "lr_values = []\n",
    "rewards = []\n",
    "for _, row in results_df.iterrows():\n",
    "    if 'hyperparameters' in row and row['hyperparameters']:\n",
    "        lr = row['hyperparameters'].get('learning_rate', None)\n",
    "        if lr:\n",
    "            lr_values.append(lr)\n",
    "            rewards.append(row['mean_reward'])\n",
    "\n",
    "if lr_values:\n",
    "    ax4.scatter(lr_values, rewards, s=100, alpha=0.6, c=rewards, cmap='RdYlGn')\n",
    "    ax4.set_xlabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Mean Reward', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Impact of Learning Rate on Performance', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xscale('log')\n",
    "    ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = plots_dir / \"dqn_comparison.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ Comparison plot saved to {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 10: Detailed Analysis - Top 3 Configurations\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Detailed analysis of top 3 performing configurations\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 3 CONFIGURATIONS - DETAILED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "top3 = results_df.nlargest(3, 'mean_reward')\n",
    "\n",
    "for idx, (_, config) in enumerate(top3.iterrows(), 1):\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"#{idx}: {config['config_name']}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"Mean Reward: {config['mean_reward']:.2f} Â± {config['std_reward']:.2f}\")\n",
    "    print(f\"Success Rate: {config['success_rate']*100:.1f}%\")\n",
    "    print(f\"Avg Episode Length: {config['mean_episode_length']:.1f} Â± {config['std_episode_length']:.1f} steps\")\n",
    "    print(f\"Training Time: {config['training_time']:.1f} seconds\")\n",
    "    \n",
    "    if 'hyperparameters' in config and config['hyperparameters']:\n",
    "        print(f\"\\nKey Hyperparameters:\")\n",
    "        hparams = config['hyperparameters']\n",
    "        print(f\"  - Learning Rate: {hparams.get('learning_rate', 'N/A')}\")\n",
    "        print(f\"  - Batch Size: {hparams.get('batch_size', 'N/A')}\")\n",
    "        print(f\"  - Buffer Size: {hparams.get('buffer_size', 'N/A')}\")\n",
    "        print(f\"  - Gamma: {hparams.get('gamma', 'N/A')}\")\n",
    "        print(f\"  - Exploration Fraction: {hparams.get('exploration_fraction', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 11: Export Summary for Report\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Create summary tables and statistics for the final report\n",
    "\"\"\"\n",
    "\n",
    "# Summary statistics\n",
    "summary = {\n",
    "    'total_configs': len(all_results),\n",
    "    'best_config': best_config['config_name'],\n",
    "    'best_mean_reward': float(best_config['mean_reward']),\n",
    "    'best_success_rate': float(best_config['success_rate']),\n",
    "    'avg_mean_reward': float(results_df['mean_reward'].mean()),\n",
    "    'std_mean_reward': float(results_df['mean_reward'].std()),\n",
    "    'avg_success_rate': float(results_df['success_rate'].mean()),\n",
    "    'total_training_time': float(results_df['training_time'].sum()),\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = results_dir / \"dqn_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DQN TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nâœ“ Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd399068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 12: Instructions for Next Steps\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "Next steps after DQN training\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. âœ“ DQN Training Complete!\")\n",
    "print(f\"   - Trained {len(all_results)} configurations\")\n",
    "print(f\"   - Best model: {best_config['config_name']}\")\n",
    "print(f\"   - Success rate: {best_config['success_rate']*100:.1f}%\")\n",
    "\n",
    "print(\"\\n2. â†’ Train Other Algorithms:\")\n",
    "print(\"   - PPO (Policy Gradient)\")\n",
    "print(\"   - A2C (Actor-Critic)\")\n",
    "print(\"   - REINFORCE\")\n",
    "\n",
    "print(\"\\n3. â†’ Compare All Algorithms:\")\n",
    "print(\"   - Create comparison plots\")\n",
    "print(\"   - Analyze which works best\")\n",
    "print(\"   - Write discussion section\")\n",
    "\n",
    "print(\"\\n4. â†’ Create Demonstration:\")\n",
    "print(\"   - Record video with best model\")\n",
    "print(\"   - Show Unity visualization\")\n",
    "print(\"   - Explain agent behavior\")\n",
    "\n",
    "print(\"\\n5. â†’ Files Generated:\")\n",
    "print(f\"   - Models: {base_dir}\")\n",
    "print(f\"   - Results: {csv_path}\")\n",
    "print(f\"   - Plots: {plot_path}\")\n",
    "print(f\"   - Summary: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ready to train PPO next? ğŸš€\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
